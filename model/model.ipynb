{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import importlib\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from model_utils import *\n",
    "from ModelInfo import ModelInfo\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.utils import all_estimators\n",
    "from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des modèles sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif étant de comparer les performances de différents modèles, nous proposons une approche comparant tous les modèles régressif de la bibliothèque scikit-learn, ainsi que d'autres modèles souvent utilisés pour de tâches de régression (ex: xgboost). \\\n",
    "/!\\ La comparaison de ces modèles n'a de sens que si l'on comprend le fonctionnement interne de chacun des modèles de la librairie. \\\n",
    "L'approche exploratoire proposée ici est un challenge ajouté au projet. Nous ne retiendrons que les résultats des modèles les plus connus pour répondre à la problématique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Récupération des modèles régression de sklearn - classement par groupes custom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_estimators_list = all_estimators(type_filter='regressor')\n",
    "\n",
    "# catégories de modèles\n",
    "model_categories = {\n",
    "    \"Linear Models\": [],\n",
    "    \"Tree-based Models\": [],\n",
    "    \"Ensemble Methods\": [],\n",
    "    \"Bayesian Models\": [],\n",
    "    \"Support Vector Machines\": [],\n",
    "    \"K-Nearest Neighbors\": [],\n",
    "    \"Neural Networks\": [],\n",
    "    \"Others\": []\n",
    "}\n",
    "\n",
    "for name, EstimatorClass in all_estimators_list:\n",
    "    try:\n",
    "        model_instance = EstimatorClass()\n",
    "        model_description = EstimatorClass.__doc__.split(\"\\n\")[0] if EstimatorClass.__doc__ else \"No description\"\n",
    "    except TypeError:\n",
    "        model_description = \"Requires additional arguments\"\n",
    "\n",
    "    # On filtre par catégories de modèles\n",
    "    if issubclass(EstimatorClass, LinearRegression):\n",
    "        model_categories[\"Linear Models\"].append((name, model_description))\n",
    "    elif issubclass(EstimatorClass, DecisionTreeRegressor):\n",
    "        model_categories[\"Tree-based Models\"].append((name, model_description))\n",
    "    elif issubclass(EstimatorClass, RandomForestRegressor):\n",
    "        model_categories[\"Ensemble Methods\"].append((name, model_description))\n",
    "    elif 'Bayes' in name:\n",
    "        model_categories[\"Bayesian Models\"].append((name, model_description))\n",
    "    elif 'SVR' in name:\n",
    "        model_categories[\"Support Vector Machines\"].append((name, model_description))\n",
    "    elif 'KNeighbors' in name:\n",
    "        model_categories[\"K-Nearest Neighbors\"].append((name, model_description))\n",
    "    elif 'MLP' in name:\n",
    "        model_categories[\"Neural Networks\"].append((name, model_description))\n",
    "    else:\n",
    "        model_categories[\"Others\"].append((name, model_description))\n",
    "\n",
    "# modèles + descriptions\n",
    "for category, models in model_categories.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for model_name, description in models:\n",
    "        print(f\" - {model_name}: {description}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Récupération des modèles régression de sklearn - classement par modules sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_estimators_list = all_estimators(type_filter='regressor')\n",
    "regressor_models_by_module = defaultdict(list)\n",
    "\n",
    "for name, EstimatorClass in all_estimators_list:\n",
    "    try:\n",
    "        model_instance = EstimatorClass()\n",
    "        model_description = EstimatorClass.__doc__.split(\"\\n\")[0] if EstimatorClass.__doc__ else \"No description\"\n",
    "    except TypeError:\n",
    "        # Si l'initialisation échoue = nécessite des arguments supplémentaires\n",
    "        model_description = \"Requires additional arguments\"\n",
    "    \n",
    "    module = EstimatorClass.__module__.split('.')[1]  # ex: 'linear_model', 'tree'\n",
    "    regressor_models_by_module[module].append((name, EstimatorClass, model_description))\n",
    "\n",
    "\n",
    "def print_regressor_models_by_module(models_by_module):\n",
    "    for module, models in models_by_module.items():\n",
    "        print(f\"\\nModule: {module}\")\n",
    "        for model_name, _, description in models:\n",
    "            print(f\" - {model_name}: {description}\")\n",
    "\n",
    "print_regressor_models_by_module(regressor_models_by_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test de séparation des modèles supportant le multi-output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeu de données de régression avec plusieurs sorties (multi-output)\n",
    "X, y = make_regression(n_samples=100, n_features=4, n_targets=2, noise=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "all_estimators_list = all_estimators(type_filter='regressor')\n",
    "\n",
    "\n",
    "regressor_models_by_module = defaultdict(list)\n",
    "multioutput_models = defaultdict(list)\n",
    "non_multioutput_models = defaultdict(list)\n",
    "\n",
    "\n",
    "for name, EstimatorClass in all_estimators_list:\n",
    "    try:\n",
    "        model_instance = EstimatorClass()\n",
    "        model_description = EstimatorClass.__doc__.split(\"\\n\")[0] if EstimatorClass.__doc__ else \"No description\"\n",
    "        model_instance.fit(X_train, y_train) # Multi-output\n",
    "        # Si l'entraînement réussit, on considère que le modèle supporte le multi-output\n",
    "        multioutput_models[EstimatorClass.__module__.split('.')[1]].append((name, EstimatorClass, model_description))\n",
    "    except (TypeError, ValueError):\n",
    "        # Sinon, des arguments sont nécessaires ou le modèle ne supporte pas le multi-output\n",
    "        non_multioutput_models[EstimatorClass.__module__.split('.')[1]].append((name, EstimatorClass, model_description))\n",
    "        continue\n",
    "\n",
    "\n",
    "print(\"\\n=== Models that natively support multi-output ===\")\n",
    "print_regressor_models_by_module(multioutput_models)\n",
    "\n",
    "print(\"\\n=== Models that do not support multi-output ===\")\n",
    "print_regressor_models_by_module(non_multioutput_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Chargement des modèles sklearn + externes via la classe ModelInfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de la classe custom ModelInfo avec un modèle externe comme XGBoost\n",
    "external_models = [\n",
    "    ('XGBRegressor', xgb.XGBRegressor)\n",
    "]\n",
    "\n",
    "model_info = ModelInfo(external_models=external_models)\n",
    "model_info.fill_dataframe()\n",
    "\n",
    "model_info.df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model_info.df_models.to_dict('records')\n",
    "\n",
    "df_models = pd.DataFrame(data)\n",
    "bool_columns = df_models.select_dtypes(include=[bool])\n",
    "\n",
    "plt.figure(figsize=(10, 16))\n",
    "sns.heatmap(bool_columns, annot=True, cmap='coolwarm', cbar=False, yticklabels=df_models['model_name'])\n",
    "plt.title(\"Représentation des modèles et de leurs propriétés booléennes\")\n",
    "plt.xlabel(\"Propriétés\")\n",
    "plt.ylabel(\"Modèles\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test des modèles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Quality_group.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = make_regression(n_samples=100, n_features=4, n_targets=2, noise=0.1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# load the dataset Quality_group.csv\n",
    "df = pd.read_csv('../data/Quality_group.csv')\n",
    "target_labels = ['Yield strength / MPa', \n",
    "                 'Ultimate tensile strength / MPa', \n",
    "                 'Elongation / %', \n",
    "                 'Reduction of Area / %']\n",
    "# target_labels = ['Charpy impact toughness / J',\n",
    "#                  'Charpy temperature']\n",
    "\n",
    "\n",
    "X,y = df.drop(target_labels, axis=1), df[target_labels]\n",
    "\n",
    "# On retire les colonnes one-hot encoded pour le PCA\n",
    "one_hot_encoded_cols = [col for col in X.columns if X[col].nunique() <= 2 and set(X[col].unique()).issubset({0, 1})]\n",
    "non_one_hot_cols = X.drop(columns=one_hot_encoded_cols).select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[non_one_hot_cols])\n",
    "X_test_scaled = scaler.transform(X_test[non_one_hot_cols])\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=3)  \n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "X_train_pca_df = pd.DataFrame(X_train_pca, columns=[f'PCA_{i+1}' for i in range(X_train_pca.shape[1])])\n",
    "X_test_pca_df = pd.DataFrame(X_test_pca, columns=[f'PCA_{i+1}' for i in range(X_test_pca.shape[1])])\n",
    "X_train_final = pd.concat([X_train.reset_index(drop=True), X_train_pca_df], axis=1)\n",
    "X_test_final = pd.concat([X_test.reset_index(drop=True), X_test_pca_df], axis=1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# # standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# pca = PCA(n_components=3) \n",
    "# X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "# X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def perform_random_grid_search(model, param_distributions, X, y, n_iter=10, cv=3, scoring='neg_mean_squared_error'):\n",
    "    \"\"\"\n",
    "    Effectue une recherche aléatoire sur une grille de paramètres pour un modèle donné.\n",
    "    Args:\n",
    "        model: Estimator object.\n",
    "        param_distributions: Dictionary with parameters names (string) as keys and distributions or lists of parameters to try.\n",
    "        X: Training data.\n",
    "        y: Target variable.\n",
    "        n_iter: Number of parameter settings that are sampled.\n",
    "        cv: Cross-validation splitting strategy.\n",
    "        scoring: Scoring method.\n",
    "    \"\"\"\n",
    "    random_search = RandomizedSearchCV(model, param_distributions, n_iter=n_iter, cv=cv, scoring=scoring, random_state=42)\n",
    "    random_search.fit(X, y)\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "    \n",
    "def train_and_evaluate_models_cv(df_models, X, y, random_grid_search=False, param_grids=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Entraîne et évalue des modèles de régression avec validation croisée.\n",
    "    Args:\n",
    "        df_models: DataFrame with models information.\n",
    "        X: Training data.\n",
    "        y: Target variable.\n",
    "        random_grid_search: Perform random grid search for hyperparameter tuning.\n",
    "        param_grids: Dictionary with hyperparameter grids for each model.\n",
    "        verbose: Print additional information.\n",
    "    Returns:\n",
    "        DataFrame with evaluation results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Avantages de make_scorer:\n",
    "    # - Permet de personnaliser les métriques de scoring\n",
    "    # - Permet de spécifier si une métrique doit être maximisée ou minimisée\n",
    "    # - cross_validate et GridSearchCV prennent en charge les objets make_scorer\n",
    "    scoring = {\n",
    "        'MSE': make_scorer(mean_squared_error),\n",
    "        'MAE': make_scorer(mean_absolute_error),\n",
    "        'R2': make_scorer(r2_score)\n",
    "    }\n",
    "    \n",
    "    for idx, row in tqdm(df_models.iterrows(), total=df_models.shape[0], desc=\"Training Models\"):\n",
    "        try:\n",
    "            name = row['model_name']\n",
    "            EstimatorClass = row['estimator']  # Référence à la classe de l'estimateur\n",
    "            multi_output_native = row['multi_output_native']\n",
    "            multi_output_compatible = row['multi_output']\n",
    "            \n",
    "            # modèle natif multi-output\n",
    "            if multi_output_native:\n",
    "                model_instance = EstimatorClass()\n",
    "            # modèle compatible avec MultiOutputRegressor\n",
    "            elif multi_output_compatible:\n",
    "                model_instance = MultiOutputRegressor(EstimatorClass())\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Grid search\n",
    "            if random_grid_search and param_grids and name in param_grids:\n",
    "                tqdm.write(f\"Performing Random Grid Search for {name}...\")\n",
    "                model_instance = perform_random_grid_search(model_instance, param_grids[name], X, y)\n",
    "            \n",
    "            # Validation croisée\n",
    "            cv_results = cross_validate(model_instance, X, y, cv=5, scoring=scoring, return_train_score=True)\n",
    "            \n",
    "            # Scores\n",
    "            train_mse = np.mean(cv_results['train_MSE'])\n",
    "            val_mse = np.mean(cv_results['test_MSE'])\n",
    "            train_mae = np.mean(cv_results['train_MAE'])\n",
    "            val_mae = np.mean(cv_results['test_MAE'])\n",
    "            train_r2 = np.mean(cv_results['train_R2'])\n",
    "            val_r2 = np.mean(cv_results['test_R2'])\n",
    "            \n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'Description': row['description'],\n",
    "                'Train MSE': train_mse,\n",
    "                'Validation MSE': val_mse,\n",
    "                'Train MAE': train_mae,\n",
    "                'Validation MAE': val_mae,\n",
    "                'Train R2': train_r2,\n",
    "                'Validation R2': val_r2\n",
    "            })\n",
    "\n",
    "            if name == 'XGBRegressor':\n",
    "                model_instance.fit(X, y)\n",
    "                feature_importances = model_instance.feature_importances_\n",
    "                feature_names = X.columns\n",
    "                feature_importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "                feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "                # On garde les 15 premières features et on regroupe les autres dans 'Others'\n",
    "                top_15 = feature_importances_df.head(15)\n",
    "                others = pd.DataFrame({\n",
    "                    'Feature': ['Others ({})'.format(len(feature_importances_df) - 15)], \n",
    "                    'Importance': [feature_importances_df.iloc[15:]['Importance'].sum()]\n",
    "                })\n",
    "                final_df = pd.concat([top_15, others])\n",
    "\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.barplot(x='Importance', y='Feature', data=final_df, palette='rocket')\n",
    "                plt.title(f\"Feature Importance - {name}\")\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Failed to evaluate model {name}:\")\n",
    "            if verbose:\n",
    "                tqdm.write(str(e))\n",
    "            # traceback.print_exc()  # Pour le détail des erreurs\n",
    "            continue\n",
    "    \n",
    "    # Retourner les résultats sous forme de DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Exemple de grille de paramètres pour certains modèles (pour Randomized Grid Search)\n",
    "# --------------------------\n",
    "# Pour l'instant, le grid search n'est pas randomisé, on donne juste des valeurs à tester\n",
    "# --------------------------\n",
    "param_grids = {\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "df_results = train_and_evaluate_models_cv(df_models, X_train_final, y_train, random_grid_search=True, param_grids=param_grids)\n",
    "df_results = df_results.sort_values(by='Validation R2', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des résultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_heatmap(df_results):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    heatmap_data = df_results[['Model', 'Validation MSE', 'Validation MAE', 'Validation R2']].set_index('Model')\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='rocket', cbar_kws={'label': 'Performance'}, fmt='.3f')\n",
    "\n",
    "\n",
    "    plt.title('Performance des modèles triée par Validation R2', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_results_heatmap(df_results)\n",
    "\n",
    "def plot_results_barh(df_results, metric='Validation R2', ylim=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_results = df_results.sort_values(by=metric, ascending=False)\n",
    "    sns.barplot(x=metric, y='Model', data=df_results, palette='rocket')\n",
    "    plt.title(f'Modèles triés par {metric}', fontsize=14)\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel('Modèle')\n",
    "    # set lims\n",
    "    if ylim:\n",
    "        plt.xlim(ylim)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_results_barh(df_results, metric='Validation R2', ylim=(0, 1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
